%!TEX root = twig-gpu.tex

\section{Accelerator Programming}

Programming for GPUs can be a challenging task, in large part due to the
partitioned memory model that they impose on programmers. Unlike a basic SMP,
data must be explicitly moved within the memory hierarchy such that the
appropriate processing device can access it. There is a considerable amount of
protocol that must be invoked in order to set up and release the device, move
data to and from the device, and synchronize processing at barriers. This
protocol logic becomes more complex when tuning for performance by using
asynchronous memory transfers to overlap computation and data movement. Yet
for many problems, the \emph{computational logic} of a program related to the
goals of the application is fairly simple to state -- perform some set of
functions on vector or array data. Current GPU programming techniques in
common languages for high performance computing (such as C, C++, and Fortran)
require this computational logic to be intermixed with the \emph{protocol
logic}, resulting in programs that are complex to write, maintain, and tune
for performance. Composition of independently developed program units that use
the GPU is similarly complex due to limited (or nonexistent) methods for
reasoning about the result of this composition in terms of lower level memory
usage and task creation.

This pattern suggests that programmers should consider abstracting the
interface to the GPU that constitutes the protocol logic of the program, so
that they can focus on the domain oriented computational logic of the program.
This will allow them to avoid obfuscating their computational logic with the
tedious details of interacting with the device. There are numerous ways to
approach this problem.

First, programmers could build a library of functions or objects which hide as
many details as possible. In fact this has already been done to a large
extent, with frameworks such as CUDA or OpenCL, and these libraries are the
primary way in which programmers already program for GPUs. This approach
mitigates many difficult issues, but provides only a minimal abstraction --
the library abstraction cannot hide the need to coordinate GPU activities such
as set up and memory management. Programmers might try to create their own
libraries on top of CUDA or OpenCL, but in most languages the object or
library facilities will not allow them to reason at a high semantic level. In
other words, they might be able to simplify some operations by specializing
them (e.g. ignore the possibility for multiple devices), but they cannot hide
them altogether.

Another approach is to design a domain-specific language for GPUs. Examples
include PyCUDA and OpenMPC. Domain-specific languages are typically not
customizable for higher-level application logic.

A technique that is becoming popular and is based on the OpenMP model of
directive-based code annotations hides much of the protocol logic by leaving
it to a compiler and runtime system to implement it. The PGI
Accelerate~\cite{pgi-accelerate} model and the HMPP programming
system~\cite{hmpp} are examples of this in compiler products. This approach
will, like OpenMP, make significant progress on lowering the barrier of entry
for programmers. Unfortunately, these methods provide too much a black-box to
the programmer, limiting their ability to customize the underlying GPU
protocol logic.

\subsection{The Twig Approach}

% What is the twig approach?  Give a little info here.

A key milestone in the history of programming languages was the introduction
of high level types into early languages. The ability of programmers to
separate the representation of a value of different types (e.g., floating
point versus integer; record types; arrays) allowed program code to be written
in terms of the mathematical abstractions that bits in memory represented
without exposing how the values were laid out in memory. We believe that a
similar type-level abstraction is possible in modern hybrid computer
architectures. The abstraction that must be moved to the type system is that
of the location of data within the system.

Changes in representation of basic values, such as integers or floats, is
often as simple as a type casting operator or an implicit type conversion as
allowed by the language. If residence information about data is part of the
type system, then movement of data within the memory hierarchy of a hybrid
machine can also be made as simple to express as traditional type coercions.
For the domain of GPGPU programming, Twig allows a domain specific type system
to be created in which this additional location information can be added to
the type of variables with corresponding type coercion operators that map to
lower level memory movement logic that defines the protocol between the host
and GPU device.

In addition, by abstracting the protocol operations away to high level type
coercion operators, Twig can support automated reasoning about programs that
allows optimizations to be performed related to the underlying protocol logic.
For example, if a programmer defines a sequence of statements that, based on
their type, state that data will move back and forth between the host and
device without modification on the host side, unnecessary copies of data can
be removed. This will increase program performance by reducing unnecessary
burden on the memory subsystem.

Current systems like CUDA and OpenCL allow the programmer to explicitly
allocate memory on one of the many devices that control some portion of the
memory within a machine, but require copies to be explicitly implemented by
the programmer. The result is fine grained control of the machine at the cost
of limited analysis and optimization of data movement within the memory
hierarchy. This makes writing complex programs difficult, particularly with
respect to performance. We believe that a Twig-based DSL for GPU programming
based on extending the type system of existing languages is a step towards
removing this burden from the programmer.
