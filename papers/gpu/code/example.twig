-- This example ignores some important stuff:
-- 1. Things like SIZE and N are treated as constant parameters; they could  
--    instead be passed as data through tuples, or as block parameters.
-- 2. No cleanup (free, cudaFree, etc.). This can be handled by closing blocks, 
--    but right now this is C specific and I don't have a good syntax for it.
-- 3. We don't have the CPU wait for the GPU before copying back. This would 
--    actually work fine, but to do it properly we would want to make a separate 
--    rule, and include more reductions to move and combine redundant barriers.

copyArrayToGPU = [array(float) -> gpu(array(float))] {
  cudaMalloc((void **) &$out, SIZE);
  cudaMemcpy($out, $in, SIZE, cudaMemcpyHostToDevice);
}

copyArrayFromGPU = [gpu(array(float)) -> array(float)] {
  $out = malloc(SIZE * sizeof(float));
  cudaMemcpy($out, $in, SIZE, cudaMemcpyHostToDevice);
}

kernelA = [gpu(array(float)) -> gpu(array(float))] {
  times_two <<< N_BLOCKS, BLOCK_SIZE >>> ($in, N);
  $out = $in;
}

kernelB = [gpu(array(float)) -> gpu(array(float))] {
  plus_one <<< N_BLOCKS, BLOCK_SIZE >>> ($in, N);
  $out = $in;
}

runKernel(k) = copyArrayToGPU;k;copyArrayFromGPU

main = runKernel(kernelA);runKernel(kernelB)

@reduce copyArrayFromGPU;copyArrayToGPU -> id
