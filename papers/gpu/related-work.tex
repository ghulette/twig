%!TEX root = twig-gpu.tex

\section{Related Work}

% \subsection{Accelerator Programming}
% 
% Programmers could build a library of functions or objects which hide as many
% details as possible. In fact this has already been done to a large extent,
% with frameworks such as CUDA or OpenCL, and these libraries are the primary
% way in which programmers already program for GPUs. This approach mitigates
% many difficult issues, but provides only a minimal abstraction -- the library
% abstraction cannot hide the need to coordinate GPU activities such as set up
% and memory management. Programmers might try to create their own libraries on
% top of CUDA or OpenCL, but in most languages the object or library facilities
% will not allow them to reason at a high semantic level. In other words, they
% might be able to simplify some operations by specializing them (e.g. ignore
% the possibility for multiple devices), but they cannot hide them altogether.
% 
% Another approach is to design a domain-specific language for GPUs. Examples
% include PyCUDA and OpenMPC. Domain-specific languages are typically not
% customizable for higher-level application logic.
% 
% A technique that is becoming popular and is based on the OpenMP model of
% directive-based code annotations hides much of the protocol logic by leaving
% it to a compiler and runtime system to implement it. The PGI
`% Accelerate~\cite{pgi-accelerate} model and the HMPP programming
% system~\cite{hmpp} are examples of this in compiler products. This approach
% will, like OpenMP, make significant progress on lowering the barrier of entry
% for programmers. Unfortunately, these methods provide too much a black-box to
% the programmer, limiting their ability to customize the underlying GPU
% protocol logic.

\comment{This section needs serious rewrites.}

The closest work to that which we describe is Reppy's Application
Specific Foreign Function Interface Generator,
FIG~\cite{reppy06fig}. In that case, a similar formal approach was
taken specifically in relation to the generation of bindings between
programs in two different programming languages.  Very similar issues
arise in building bindings between languages that occur between
devices in a hybrid system.  These overlaps include memory ownership
and management, data marshalling, and managing the flow of program
control across the language or device boundary.  Our work builds upon
that of Reppy and Song, and aims to provide a more generalizable
method that focuses solely on type-level mappings. These mappings
subsume the FFI generation case (in which mappings are between two
different type systems), and other interesting applications like
mapping between two different libraries written in the same
language. This second case is of particular interest when dealing with
the problem of composing complex software from smaller program units,
where developers working independently may have chosen different
representations for data types that are semantically identical.

Kennedy's \emph{telescoping languages} work is also related to ours in
that it sought to support high-level programming by defering
challenging problems to automated tools.  In particular, the
telescoping languages effort sought to provide the ability to build
high-level problem-solving languages that used scripting languages to
coordinate functionality present in domain-specific
libraries~\cite{kennedy00telescoping}. Much of the work then focused
on compiler optimization methods and targeting potentially
distributed, grid-based environments. Interestingly, one of the
compilation techniques that was called out as part of the telescoping
languages strategy was that of automated recognition and exploitation
of identities. This would allow a compilation tool to recognize
instances when compositions of functions could be replaced with more
efficient equivalent implementations. As is described in this paper,
we adopt a similar strategy in identifying compositions of rules that
are equivalent to the identity function and can therefore be
eliminated.

Domain-specific language approaches have had notable success in the
computational science field, an exemplar being the Tensor Contraction
Engine (TCE)~\cite{baumgartner05synthesis}. The TCE allows
computational chemists to write tensor contraction operations in a
high level language similar to Mathematica, leaving it up to the TCE
tool to generate the corresponding collections of loops that implement
the operations. The advantage of this approach is that tedious and
often error-prone nested loops over many large arrays with a
correspondingly large number of indices can be both machine generated
and optimized. Optimizations such as loop fusion, memory locality
management, and data distribution and partitioning in a parallel
machine can all be automated, versus previous approaches that required
very labor intensive hand written code. The TCE is not a general tool,
and is only of use to programmers working with similar tensor-based
computations.


% With respect to the GPU programming context specifically, a number of language
% approaches have been investigated to alleviate programmers of the tedious
% burden of programming in languages like CUDA or OpenCL.	For example, ...
% 
% 
% Discuss PyCUDA and OpenMPC. What about Jacket and other GPU-type interfaces
% for languages like Matlab?
% 
% HPF was interesting because it explored extending the type system of Fortran
% 90 to add data distribution information, which feels similar to the Twig GPU
% idea. definitely need to talk about that at some point in the paper.
% 
% Other general-purpose DSLs? Yampa/Arrows?
% 
% Accelerate GPU language in Haskell is similar.	
