%!TEX root = twig-gpu.tex

\section{Introduction}

Programming for hybrid computers can be a challenging task, in large
part due to the partitioned memory model that they impose on
programmers. Unlike a basic SMP, data must be explicitly allocated on
different devices and moved within the memory hierarchy of a single
compute node such that the appropriate processing device can access
it. This induces a considerable amount of protocol when adopting
programming models like OpenCL or CUDA that must be explicitly defined
by the programmer in order to set up and release the device, allocate
space and move data to and from the device, and synchronize processing
at control flow barriers that logically span devices. This protocol
logic becomes more complex when tuning for performance by minimizing
the number of data transfers that occur and using asynchronous memory
transfers to overlap computation and data movement.  In many
standardized programming systems that are used for programming hybrid
computers, there is an intermingling of computational, application
logic with this platform-oriented protocol logic.  This leads to
excessively complex and tedious software that is prohibitively
difficult to develop, maintain, and compose with other software.

\subsection{Protocol vs. Computational Logic}

For many problems, the \emph{computational logic} of a program related
to the goals of the application is fairly simple to state -- perform
some set of functions on vector or array data. Current accelerator
programming techniques in common languages for high performance
computing (such as C, C++, and Fortran) require this computational
logic to be intermixed with the \emph{protocol logic} to varying
degrees, resulting in programs that are complex to write, maintain,
and tune for performance.  Composition of independently developed
program units that use the hybrid architectures is similarly complex
due to limited (or nonexistent) methods for reasoning about the
product of composition inn terms of lower level memory usage and task
creation.  In this paper, we will focus on the specific instance of this
problem presented by GPU-based acclerators.

We hypothesize that one can avoid the complexity that this intermingling
of comptational and protocol logic without resorting to specialized
programming systems such as vendor-specific directive methods like the
PGI Accelerator or HMPP systems.  An abstraction is possible by
extending the type system of a traditional language such that protocol logic
can be turned into a problem of reasoning about types alone.  

%This pattern suggests that programmers should consider abstracting the
%interface to the GPU that constitutes the protocol logic of the
%program, so that they can focus on the domain oriented computational
%logic of the program. This will allow them to avoid obfuscating their
%computational logic with the tedious details of interacting with the
%device.  \comment{This paragraph is weak}

It is crucial that GPU programming systems are able to integrate with
existing code and programming tools. Scientists in the national
laboratories, for example, typically write code in popular
general-purpose languages such as C/C++, Fortran, or Python. They also
make frequent use of the vast catalog of libraries in these
languages. Domain-specific accelerator languages which cannot
integrate, or integrate only with difficulty, with existing codes in
these languages are of limited use in these scenarios.  Instead, a
system that imposes a minimal and orthogonal extension to the type
system of an existing language can aid users of existing languages
without being forced to adopt a single vendor-defined model.

\subsection{The Twig Approach}

In this paper we present a high-level code generation language called
\emph{Twig} and show how it can be used to overcome some of the
obstacles to GPU accelerator programming described above. Twig allows
for simple, high-level GPU programming, and at the same time supports
automated reasoning about composite programs that can, in many cases,
avoid redundant memory copying and thereby retain high
performance. Crucially, Twig's role in the programming toolchain is to
generate code in a mainstream language like C. The generated code is
then incorporated into a surrounding program, which is built as
usual. This minimizes the complexity that Twig adds to the build
process, and allows Twig code to interact easily with existing code
and libraries.  Furthermore, by adopting a code generation approach,
the resultant code induces no additional dependency on a third-party
tool since the generated code can be integrated directly into the code
base, requiring only the compilation tools of the base language and
standardized libraries (such as OpenCL) for supporting hybrid
processing.

Twig achieves these goals by leveraging the theory of term
rewriting~\cite{baader98rewriting}, along with a scheme which uses
data types to direct the generation of code in the target language. In
particular, we augment existing data types in the target language with
a notion of \emph{location}, e.g., an array of floats located on a
GPU, or an integer located in main memory.  In this paper, we do not
present a full toolchain, but instead focus on the crucial aspect of
Twig upon which this work relies - the translation to and from a term
representation of types and operations on these types, and the code
generation techniques used to map the result of term manipulation to
the base language and application code.  We describe the basics of
Twig's language in the following section, and then discuss how we use
this language along with located types to generate code for GPUs.  We
also show how optimization can be performed for data movement between 
devices via term rewriting formalisms.

% Writing code for accelerators such as GPUs can be tedious and error prone. One
% problem is that the dominant methods for programming in these environments,
% such as CUDA or OpenGL, provide only a low-level interface to the hardware.
% For many programmers, in particular domain programmers such as simulation
% scientists, a higher-level interface may be desired. The trade-off, however,
% may be a pronounced decrease in performance. Since performance is generally
% the salient reason to use an accelerator in the first place, this situation
% represents a major impediment to the widespread adoption of accelerator
% hardware environments by non-specialist programmers.

% One of the major potential impediments to performance on accelerators, of
% particular hazard to high-level programming models, is redundant memory
% movement. Accelerators typically possess their own special-purpose memory,
% separate from the main system memory. Programmers must explicitly copy data
% from the system to the accelerator before it can be processed there. Once the
% data is processed, it must be copied back into the system memory. Copying
% memory to and from the device is typically quite slow, presenting one of the
% major bottlenecks for accelerator performance. Programmers must be careful,
% therefore, to copy data only as needed, and to do as much processing as
% possible on the device before copying the data back to the system.

% Why C? Because it is lingua franca, and it is the language of accelerator %
% APIs.
