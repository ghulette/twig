%!TEX root = twig.tex

\subsection{Language-neutral intermediate representations}
\label{neutral_ir}

Language-neutral intermediate representations (neutral IRs) are, in some sense, similar to IDL-based approaches. Both work by constructing a ``common ground'' that participating languages must be able to interface with. This approach connects each language, by transitivity, to every other language that targets the same IR.

In the language-neutral IR approach, each language is compiled to some target language (the IR) that is general enough to encode the data representations and semantics for a variety of languages. The IR serves as the mechanism for interoperability. The nature of the IR determines how interoperability mechanisms are exposed in each languages. Neutral IRs, then, can be seen as providing a framework or mechanism for multi-language interoperability, on top of which other strategies (including FFIs or language integration) can be applied.

%%
%% EXAMPLES
%%

\subsubsection{Examples}

The following systems all feature multi-language interoperability facilitates by a language-neutral IR.

\paragraph{UNCOL}

As far back as 1958, there was interest in developing a ``universal compiler IR,'' called UNCOL~\cite{macrakis93andf}. UNCOL was more a concept than an actual proposal, and it was never successfully designed. The idea was proposed as a means to reduce the effort required to write compilers, which was at the time considerable. Machine architectures at the time were not standardized, and language features like records, pointers, and data types were still novel. A universal IR was thought to be a partial solution -- languages could target the IR, which would then be portable to many hardware architectures. Language interoperability was hardly considered, but was mentioned as a potential extra benefit.

\paragraph{Microsoft Common Language Infrastructure}

Microsoft's Common Language Runtime (CLR)~\cite{ms06cli,
hamilton03clr} is a language-neutral IR and execution
specification, similar to Java's virtual machine
bytecode~\cite{jvmspec}, but designed to support many different
languages. It is one of the foundational technologies for
Microsoft's ``.NET'' platform. The CLR supports compilation and
execution of procedural, functional, or object-oriented paradigms
languages. CLR's language-neutral bytecode format is called Common
Intermediate Language (CIL), and it has a language-neutral type
system called the Common Type System (CTS).

Like the JVM, the CLR provides high level services like garbage collection, a class loader with security features, and an extensive (and language-independent) class library providing many common data structures and algorithms.

CLR programs are self-describing, with extensive annotations describing the types, fields that are read-only, and so on. These annotations are packaged along with the compiled CIL code. This allows compiled CIL code to be distributed independently of the source program that generated it, but still inspected and used by other CIL modules.

When executed, the platform-independent CIL is translated to platform-specific native code. CIL is a stack-based model, and although it retains type annotations as meta-data to be used by compilers and other tools, the execution engine ignores the types entirely.

The CIL is essentially a stack-based form of assembly language. Unlike assembly languages, however, it is not tied to a particular ISA, and is designed to be interpretable on any modern CPU.

Although similar to JVM bytecode, CIL has some important differences that facilitate its language neutrality. Unlike the JVM, unsafe CIL codes can be generated and are permitted to be executed. This enables languages like C++ that feature unsafe pointer arithmetic to interoperate. Also unlike JVM bytecode, CIL permits global variables, function pointers, and the ability to pass primitive parameters by reference. The garbage collection algorithm is required to support pinning data (see Section~\ref{sec:ffi}), which guarantees that pointers may be used on memory managed data.

The CTS provides a language-neutral type system for the CLR. There are two kinds of types in CTS: value and reference types. Value types are bit sequences (e.g., integers), allocated on the stack. The value type describes what operations are appropriate, but values do not carry their type information, so type checking must be done statically. References types are similar to object references in Java. Values of reference types carry their type information with them, and so can be checked dynamically.

Like Java, reference types are part of an inheritance hierarchy, and must inherit from exactly one parent. Value types exist outside the hierarchy. The CLR has a notion of \emph{interfaces}, which are sets of methods. Inheriting from an interface implies that the reference type implements all the methods described in that interface. Inheritance from multiple interfaces is allowed, and the sub-typing rules permit a reference type that inherits an interface to be used wherever that interface is expected.

Notably, although the CTS defines inheritance behavior, it intentionally omits method overloading. This is because different languages have different overloading rules. It is relatively easy for languages that wish to support overloading to implement it themselves by mangling method names.

To support language interoperability, CLR defines a Common Language Spec (CLS), which is a subset of the CTS type system. At a minimum, CLS compliant languages must be able to import and use CLS types, which include object types as well as a set of primitives. Notably, CLS-compliant languages need not be able to \emph{extend} object types via inheritance, or even to define new object types. CLS specifies some other constraints as well, for example it mandates an interoperable naming scheme for identifiers, and disallows architecture-specific primitive types. Compliance with CLS is not required, it guarantees interoperability with any other CLS compliant code. In some ways CLS is like an IDL, but an exceptionally rich one.

Error handling in the CLR is done through exceptions, so compliant languages are required to either handle or tolerate exceptions that are thrown to them. Exceptions are implemented by a two-pass stack unwinding: first the stack is searched for a handler, and then the second pass performs cleanup before invoking the handler. Exception information is stored in fixed-format table that precedes each method entry in the CIL format. The table contains a pointer to the handler as well as a discriminator indicating whether the handler may re-throw the exception or simply terminate. Exception handling is late-bound; no work is done until the exception is thrown, and then the stack is searched. With this system, an exception can be raised in one language and caught in another.

Targeting a particular language to the CLR can be easy or difficult, depending on how well the language's concepts map onto the CLR's infrastructure. For example, the CLR does not support nested functions, multiple inheritance, or callcc, so languages with these facilities must either omit the feature, or transform the feature into CLR-compatible terms. Moreover, in order to interoperate with other languages in the CLR, a language must support the minimum requirements of the CLS. This can be awkward. For example, support for SML required an extension to the language to support object types.

\paragraph{LLVM}

LLVM~\cite{lattner04llvm} specifies an intermediate code representation based on a 3-address, RISC-like architecture, but abstracted from actual hardware. It also provides a compiler infrastructure to transform that representation, including compilation to real hardware.

LLVM's representation is language independent; it is only slightly richer than a RISC-like assembly language. It does include a type system, but the types may be treated as annotations, and do not prevent definition of programs that ignore the type information. LLVM does not impose any particular runtime requirements on programs, and does not provide high-level runtime features (e.g., garbage collection) directly. This makes LLVM a very different kind of system than the CLR or JVM, which provide many high-level features but also usually require that compliant languages use them.

The LLVM IR has just 31 opcodes, but most have overloaded semantics based on the types of their arguments. LLVM does not permit type coercion; types must be cast explicitly to other types if desired, so language features like implicit coercions must be compiler directed. LLVM's IR uses an infinite set of virtual registers. The definition of a register always dominates its use, i.e., registers have single-assignment semantics. The IR includes load and store opcodes to access a heap. Control flow in LLVM is explicit: functions are basic blocks, and each block ends in either a branch, a return, or one of two exception opcodes (see below).

LLVM's type system is designed to be language-independent. Every
register and heap object has an explicit type, and opcodes work
differently with different types. The type system includes the
usual set of primitive types as well as four kinds of derived
types. These are pointers, arrays, structures, and functions.

LLVM is designed to support weakly-typed languages (e.g., C) so declared type information in an LLVM program may not be reliable. In particular, there is an unsafe opcode that will cast any type to any other type. The cast opcode is the only way to convert types; this implies that programs without the cast opcode are typesafe.~\footnote{Lack of the cast opcode does not prevent memory errors, such out-of-bounds memory or array accesses, from occurring.} Heap address arithmetic uses a dedicated opcode (instead of the general-purpose addition opcodes) that preserves type information.

LLVM uses a flexible, language-independent scheme for exceptions. The IR includes two special opcodes called invoke and unwind. Invoke works like a regular function call, but takes an extra basic block argument that represents an exception handler. When the unwind opcode is executed, it works up through the call stack until an invoke opcode is found. Execution then transfers control to the exception handler block of that invoke.

Type information allows a range of aggressive transformations that would not otherwise be possible, e.g., reordering fields; optimizing memory management. These can only be done with \emph{reliable} type information however, so they include an algorithm (Data Structure Analysis) that uses the declared types as speculative, and conservatively checks whether load/stores are consistent.


\paragraph{Moby}
\label{bol}

Moby is a functional programming language that supports interoperability with C through its compiled intermediate representation, called BOL~\cite{fisher01interop, reppy06ffi}. BOL is more expressive than Moby itself; in particular, it can also be used as an IR for C. The two languages have very different features, but can interoperate using BOL.

The BOL IR framework serves as a \emph{mechanism} for interoperability, but does not define a \emph{policy}. The distinction is important. The policy determines how low-level data structures are represented and manipulated in the high-level language. The mechanism, by contrast, exists to reify the policy. Ideally, if the interoperability mechanism is both flexible and powerful, it may support many different kinds of policies.

BOL is an extended, low-level lambda calculus. It has a weak type system, designed to be almost equivalent to that of C, but lacking C's recursive types. Type constructors in BOL include enumerations, pointers, arrays, and structures. BOL code can make C function calls and work with C data types directly. No marshalling is required because there is a direct mapping from BOL's types to those of C.

Types in Moby can be defined in terms of BOL types; this is Moby's primitive types are defined. Primitive Moby functions can also be defined in BOL; this can be used, for example, to wrap the C standard library for use in Moby, since BOL can call C functions directly.

There are two separate ways to access C from Moby, representing two distinct interoperability policies. The first is an IDL-based approach. Tools are used to parse a C header file and map the function signatures to Moby's type system. The header file may include some extra annotations to disambiguate the mapping of pointer types. Stubs are generated in BOL code that call the appropriate C functions, but map the types to Moby's high-level representations. Some marshalling code, also in BOL, may be generated if needed to implement the mapping.

The second interoperability policy is called Charon, which implements a type-safe embedding of C into the Moby language using phantom types. This allows Moby to manipulate C data structures and call C functions directly, without sacrificing type safety. Charon was inspired by, and is very similar to, NLFFI~\cite{blume01nlffi}, which is discussed in Section~\ref{nlffi}.


\paragraph{Whirl}

Whirl is the IR used in the Pro64/Open64 compiler suite~\cite{whirl, murphy08open64, liao06openuh} and its many offshoots. Whirl is designed to be used as the input and output of every internal compiler pass. This makes it easy to reorder optimization passes in the compiler, which is important because the optimal order of the optimization passes with respect to some performance criteria may be different for different applications.

Whirl was designed to support a fixed set of languages, namely C, C++, Java, and FORTRAN 77, and it may be adequate for other languages as well.

Whirl code may be designated as being at a ``level'' with higher levels being closer to the source language, and lower levels closer to the machine architecture. The process of compilation, then, is a gradual translation of a program from the highest level to the lowest one. Each level is well-defined, i.e., there are constructs in the higher-level IR that must be eliminated before the level can be reduced.

At the highest level, Whirl code is very close the original language, and at this stage it is even possible to translate from Whirl back to the original language. Whirl supports this high-level representation through constructs that are specific to different language semantics. For example, Whirl contains a \texttt{DO\_LOOP} element that corresponds to Fortran's loop semantics, as well as \texttt{DO\_WHILE} and \texttt{WHILE\_DO} elements for C. At the highest level, language-specific types are preserved as well.

While Whirl does provide a language-neutral IR for the languages it supports, it would have to be extended for an additional language to participate. Moreover, Whirl does not offer any special support for interoperability between the languages it supports. For example, Whirl does not bother to define an IR representation of C code calling a Java function, since it does not accept a source language that would admit this construct in the first place.


\paragraph{SUIF}

SUIF~\cite{wilson94suif} is a compiler infrastructure that includes a flexible IR component. SUIF is particularly focused on facilitating parallelizing transformations. To detect the data dependencies required for parallel transformations, SUIF uses a fairly high-level intermediate representation. Lower-level IRs often erase high-level representations of loops and conditionals, and array accesses are expressed as pointer arithmetic, and this makes certain data dependence analyses more difficult or impossible.

The SUIF IR includes standard RISC-like operations, but also higher-level representations for various loops, conditional statements, and array accesses. The loops and conditionals representations are similar to those in an abstract syntax tree, but are designed to be independent of a particular language.

Because it preserves high-level constructs, SUIF may be a good choice for a language-independent IR. At the moment, the SUIF distribution includes front-ends for both C and Fortran.


\subsubsection{Discussion}

C is something of a \emph{lingua franca} among programming
languages, not unlike a universal intermediate IR. As we saw in
Section~\ref{sec:ffi}, most mainstream languages have some kind of
ability to interoperate with C. C enjoys this status for at least
two reasons. First, it is a highly desirable language to
interoperate with owing to the vast numbers of existing libraries
it can access. Second, C may be used for programming tasks that
high-level languages are ill-suited for, such as coding of
performance-sensitive algorithms that can take advantage of
specialized hardware. Third, C is a very low-level program
representation while remaining platform-independent. It also has
some convenient higher-level features, such as types. In some
ways, though, C makes for a poor language-neutral IR, in
particular because it lacks high-level features like garbage
collection and exceptions.

In contrast to C, frameworks like the CLR have lots of high-level
features, and support a more coherent form of interoperability
because those features can be shared across languages. By the same
token, however, requiring that these features be supported (or at
least tolerated) by participating languages may constrain language
implementations, and prevent different but useful approaches. This
in turn may limit the utility of multi-language programming,
reducing language specialization to a matter of syntax in the most
extreme scenario. In addition many higher-level systems such as
the CLR or JVM require a large, complex runtime system to support
their execution. The runtime must be ported, possibly at great
cost and/or effort, to each platform where programs will be run.

Language-neutral IRs provide a mechanism that facilitates multi-language interoperability and programming. Some systems, like the CLR, use this mechanism to provide multi-language interoperability by designing participating languages with certain unified language semantics. Other systems, like BOL and Moby, use the neutral IR as a framework for standard approaches like FFIs. Each case, therefore, meets our goals for multi-language interoperability differently, but the use of a neutral IR impacts the goal of scalability. Like IDLs, neutral IRs allow any number of languages to interact through a single common representation -- this reduces the number of required interfaces for $n$ languages to $2n$. However, the cost to add each language amounts to the cost of writing a full compiler that targets the neutral IR.

Whether a neutral IR can accommodate multi-language type safety, offer a natural programming model, and operate efficiently depends on the system in question. The CLR, for example, goes to great lengths to fulfill these goals. It largely succeeds, at the cost of restricting language features. Systems like Moby take a different approach, allowing different interoperability policies to be layered on top of BOL. These policies, then, determine whether and how interoperability goals are fulfilled.
